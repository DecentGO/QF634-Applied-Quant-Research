{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec9a286",
   "metadata": {},
   "source": [
    "# QF 634 Applied Quantitative Research\n",
    "## Homework 4 | YU LINGFENG\n",
    "#### Artificial Neural Network\n",
    "* Single Layer Perceptron\n",
    "* MultiLayer Perceptron (MLP)\n",
    "* Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "759c119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617ee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Homework 4 Requirement\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import initializers\n",
    "keras.utils.set_random_seed(5)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80540e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98c381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9995    1\n",
      "9996    1\n",
      "9997    0\n",
      "9998    1\n",
      "9999    0\n",
      "Name: Gender, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "Y = data.iloc[:,-1].values\n",
    "X = data.iloc[:,3:13]\n",
    "X['Gender']=X['Gender'].map({'Female':0,'Male':1})\n",
    "### above is used instead of a more complicated package involving -- from sklearn.preprocessing import LabelEncoder\n",
    "### converts Female -- 0, Male -- 1, i.e. hot-encoding categorical variables\n",
    "print (X['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e436f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical variable Geography\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "### Geography is transformed into France -- 1,0,0; Spain -- 0,0,1; Germany -- 0,1,0.\n",
    "### Moreover -- this encoded vector of ones-zeros is now put in first 3 cols. Credit Score pushed to 4th col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3166a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2      3    4     5    6          7    8    9    10         11\n",
       "0  1.0  0.0  0.0  619.0  0.0  42.0  2.0       0.00  1.0  1.0  1.0  101348.88\n",
       "1  0.0  0.0  1.0  608.0  0.0  41.0  1.0   83807.86  1.0  0.0  1.0  112542.58\n",
       "2  1.0  0.0  0.0  502.0  0.0  42.0  8.0  159660.80  3.0  1.0  0.0  113931.57\n",
       "3  1.0  0.0  0.0  699.0  0.0  39.0  1.0       0.00  2.0  0.0  0.0   93826.63\n",
       "4  0.0  0.0  1.0  850.0  0.0  43.0  2.0  125510.82  1.0  1.0  1.0   79084.10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert X to dataframe X1\n",
    "X1 = pd.DataFrame(X)\n",
    "X1.head()\n",
    "### Note there are 12 features including onehotencoder for the Geography feature-- \n",
    "### The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "740ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850b84b",
   "metadata": {},
   "source": [
    "We call fit_transform() method on our training data and transform() method on our test data. Each feature in the training\n",
    "set is scaled to mean 0, variance 1. In sklearn.preprocessing.StandardScaler(), centering and scaling happens independently on each feature. The fit method is calculating the mean and variance of each of the features present in the data. The transform method is transforming all the features using the respective feature's mean and variance that are calculated in the statement\n",
    "before on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f43f1db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### This is the very first step while creating NNmodel -- you can rename this model. Here we are going to create NNmodel object \n",
    "### by using a certain class of Keras named Sequential. As a part of tensorflow 2.0, Keras is now integrated with \n",
    "### tensorflow and is now considered as a sub-library of tensorflow. The Sequential class is a part of the models module \n",
    "### of Keras library which is a part of the tensorflow library now. \n",
    "### It used to be \"import tensorflow as tf; from tensorflow import keras; from tensorflow.keras import layers\"\n",
    "### See documentation at https://keras.io/guides/sequential_model/\n",
    "\n",
    "#Initialising the NN model name -- NNmodel\n",
    "NNmodel = tf.keras.models.Sequential()\n",
    "### Sequential specifies to keras that the model NNmodel is created sequentially and the output of each layer added \n",
    "### is input to the next specified layer. Note that keras Sequential is not appropriate when the model has multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34730cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a network that has 3 hidden layer together with 1 input layer and 1 output layer. \n",
    "#Adding First Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=6,activation=\"sigmoid\"))\n",
    "#Adding Second Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=6,activation=\"sigmoid\"))\n",
    "#Adding Third Hidden Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=6,activation=\"sigmoid\"))\n",
    "### units = 6 refer to 6 neurons in hidden layer \n",
    "### modelname.add is used to add a layer to the neural network\n",
    "### -- need to specify as an argument what type of layer --\n",
    "### Dense is used to specify the fully connected layer \n",
    "### - i.e. all neurons are forward connect to all forward layer nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12edcf",
   "metadata": {},
   "source": [
    "Above -- first hidden layer is created using the Dense class which is part of the layers module. This class accepts 2 inputs:-\n",
    "(1) units:- number of neurons that will be present in the respective layer (2) activation:- specify which activation function to be used. This example uses first input as 2. There is no correct answer which is the right number of neurons in the layer -- trial and error. Not too large to be computationally impractical or redundant; not too small to be ineffective.\n",
    "For the second input, we try the sigmoid or logistic function as an activation function for hidden layers. We can also try “relu”[rectified linear unit]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "303997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we create the output layer\n",
    "#Adding Output Layer\n",
    "NNmodel.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "### Only 1 output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4a623",
   "metadata": {},
   "source": [
    "For a binary classification problem as above, actual case output is 1 or 0. Hence we require only one neuron to output layer - output could be estimated probability of case actual output = 1. For multiclass classification problem, if the output contains m categories then we need to create m different neurons, one for each category. In the binary output case, the suitable activation function is the sigmoid function. For multiclass classification problem, the activation function is typically softmax. The softmax function predicts a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70baaa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### After creating the layers -- require compiling the NNmodel. Compiling allows the computer to run and understand the program \n",
    "### without the need of more fundamental steps in the programming. Compiling adds other elements or linking other libraries, and optimization,\n",
    "### such that after compiling the results are readily computed e.g. in a binary executable program as an output. \n",
    "#Compiling NNmodel\n",
    "NNmodel.compile(optimizer=\"adam\",\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "### Note optimizer here is a more sophisticated version of the Mean Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9d245",
   "metadata": {},
   "source": [
    "Compile method above accepts inputs: (1) optimizer:- specifies which optimizer to be used in order to perform stochastic gradient descent (2) error/loss function, e.g., 'binary_crossentropy' here. For multiclass classification, it should be categorical_crossentropy, (3) metrics - the performance metrics to use in order to compute performance. 'accuracy' is one such  performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51697cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a93b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "80/80 [==============================] - 1s 860us/step - loss: 0.5893 - accuracy: 0.7972\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 687us/step - loss: 0.5327 - accuracy: 0.7972\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.5103 - accuracy: 0.7972\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 695us/step - loss: 0.5033 - accuracy: 0.7972\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.5007 - accuracy: 0.7972\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.4987 - accuracy: 0.7972\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.4964 - accuracy: 0.7972\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 772us/step - loss: 0.4937 - accuracy: 0.7972\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 763us/step - loss: 0.4902 - accuracy: 0.7972\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.4857 - accuracy: 0.7972\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 718us/step - loss: 0.4801 - accuracy: 0.7972\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.4735 - accuracy: 0.7972\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.4663 - accuracy: 0.7972\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.4589 - accuracy: 0.7972\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.4521 - accuracy: 0.7972\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.4460 - accuracy: 0.7972\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.4409 - accuracy: 0.7972\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - 0s 757us/step - loss: 0.4369 - accuracy: 0.7972\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - 0s 778us/step - loss: 0.4338 - accuracy: 0.7972\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.4315 - accuracy: 0.7972\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.4297 - accuracy: 0.7972\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.4284 - accuracy: 0.7972\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.4274 - accuracy: 0.7972\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - 0s 667us/step - loss: 0.4266 - accuracy: 0.7972\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.4256 - accuracy: 0.7972\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - 0s 688us/step - loss: 0.4248 - accuracy: 0.7974\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.4238 - accuracy: 0.8015\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - 0s 720us/step - loss: 0.4228 - accuracy: 0.8087\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.4220 - accuracy: 0.8154\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - 0s 795us/step - loss: 0.4207 - accuracy: 0.8201\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - 0s 737us/step - loss: 0.4195 - accuracy: 0.8211\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - 0s 730us/step - loss: 0.4182 - accuracy: 0.8206\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - 0s 791us/step - loss: 0.4168 - accuracy: 0.8229\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - 0s 754us/step - loss: 0.4154 - accuracy: 0.8240\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - 0s 749us/step - loss: 0.4139 - accuracy: 0.8248\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - 0s 749us/step - loss: 0.4120 - accuracy: 0.8271\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - 0s 753us/step - loss: 0.4102 - accuracy: 0.8270\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - 0s 731us/step - loss: 0.4081 - accuracy: 0.8288\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.4061 - accuracy: 0.8285\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - 0s 729us/step - loss: 0.4039 - accuracy: 0.8295\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.4019 - accuracy: 0.8286\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - 0s 724us/step - loss: 0.3998 - accuracy: 0.8291\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3978 - accuracy: 0.8307\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - 0s 779us/step - loss: 0.3958 - accuracy: 0.8300\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - 0s 755us/step - loss: 0.3939 - accuracy: 0.8324\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - 0s 714us/step - loss: 0.3920 - accuracy: 0.8335\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.3901 - accuracy: 0.8345\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - 0s 749us/step - loss: 0.3883 - accuracy: 0.8369\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3866 - accuracy: 0.8371\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3849 - accuracy: 0.8395\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3832 - accuracy: 0.8413\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3816 - accuracy: 0.8415\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3800 - accuracy: 0.8424\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3784 - accuracy: 0.8444\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.3771 - accuracy: 0.8453\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - 0s 729us/step - loss: 0.3757 - accuracy: 0.8460\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3743 - accuracy: 0.8475\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - 0s 678us/step - loss: 0.3734 - accuracy: 0.8476\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3719 - accuracy: 0.8484\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3708 - accuracy: 0.8501\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3695 - accuracy: 0.8499\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3686 - accuracy: 0.8511\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3676 - accuracy: 0.8500\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3667 - accuracy: 0.8529\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3656 - accuracy: 0.8525\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3649 - accuracy: 0.8533\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3640 - accuracy: 0.8531\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3632 - accuracy: 0.8541\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.3624 - accuracy: 0.8543\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3617 - accuracy: 0.8560\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - 0s 691us/step - loss: 0.3609 - accuracy: 0.8559\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.3604 - accuracy: 0.8568\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3597 - accuracy: 0.8579\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - 0s 812us/step - loss: 0.3589 - accuracy: 0.8575\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3583 - accuracy: 0.8583\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 687us/step - loss: 0.3576 - accuracy: 0.8586\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - 0s 726us/step - loss: 0.3573 - accuracy: 0.8574\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.3568 - accuracy: 0.8571\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3559 - accuracy: 0.8585\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - 0s 717us/step - loss: 0.3554 - accuracy: 0.8599\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3549 - accuracy: 0.8596\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3543 - accuracy: 0.8594\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3540 - accuracy: 0.8585\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - 0s 695us/step - loss: 0.3532 - accuracy: 0.8614\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.3527 - accuracy: 0.8605\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - 0s 704us/step - loss: 0.3523 - accuracy: 0.8608\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3515 - accuracy: 0.8616\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3510 - accuracy: 0.8614\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3508 - accuracy: 0.8618\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - 0s 678us/step - loss: 0.3505 - accuracy: 0.8615\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3497 - accuracy: 0.8620\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3493 - accuracy: 0.8612\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - 0s 688us/step - loss: 0.3487 - accuracy: 0.8629\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.3485 - accuracy: 0.8630\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3480 - accuracy: 0.8634\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - 0s 678us/step - loss: 0.3479 - accuracy: 0.8612\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3473 - accuracy: 0.8630\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3471 - accuracy: 0.8615\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3464 - accuracy: 0.8626\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3461 - accuracy: 0.8630\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3456 - accuracy: 0.8627\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - 0s 757us/step - loss: 0.3452 - accuracy: 0.8618\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3451 - accuracy: 0.8622\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - 0s 724us/step - loss: 0.3447 - accuracy: 0.8608\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3444 - accuracy: 0.8615\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3441 - accuracy: 0.8615\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3439 - accuracy: 0.8616\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - 0s 702us/step - loss: 0.3434 - accuracy: 0.8624\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - 0s 692us/step - loss: 0.3434 - accuracy: 0.8620\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3429 - accuracy: 0.8604\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.3428 - accuracy: 0.8622\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3417 - accuracy: 0.8614\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - 0s 715us/step - loss: 0.3422 - accuracy: 0.8619\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - 0s 694us/step - loss: 0.3417 - accuracy: 0.8621\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - 0s 708us/step - loss: 0.3414 - accuracy: 0.8627\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3410 - accuracy: 0.8620\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3413 - accuracy: 0.8614\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3406 - accuracy: 0.8633\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3403 - accuracy: 0.8621\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - 0s 715us/step - loss: 0.3402 - accuracy: 0.8605\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3398 - accuracy: 0.8629\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3398 - accuracy: 0.8629\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - 0s 686us/step - loss: 0.3394 - accuracy: 0.8622\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - 0s 682us/step - loss: 0.3390 - accuracy: 0.8615\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3389 - accuracy: 0.8621\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3388 - accuracy: 0.8615\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.3387 - accuracy: 0.8637\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - 0s 708us/step - loss: 0.3386 - accuracy: 0.8633\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3378 - accuracy: 0.8627\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.3384 - accuracy: 0.8631\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - 0s 683us/step - loss: 0.3378 - accuracy: 0.8633\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - 0s 700us/step - loss: 0.3379 - accuracy: 0.8626\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3374 - accuracy: 0.8625\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - 0s 680us/step - loss: 0.3375 - accuracy: 0.8624\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3371 - accuracy: 0.8627\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - 0s 749us/step - loss: 0.3368 - accuracy: 0.8631\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - 0s 726us/step - loss: 0.3367 - accuracy: 0.8626\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3366 - accuracy: 0.8625\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - 0s 695us/step - loss: 0.3369 - accuracy: 0.8631\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3364 - accuracy: 0.8630\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - 0s 679us/step - loss: 0.3362 - accuracy: 0.8627\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3362 - accuracy: 0.8627\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.3358 - accuracy: 0.8633\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - 0s 788us/step - loss: 0.3357 - accuracy: 0.8630\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3355 - accuracy: 0.8625\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3355 - accuracy: 0.8627\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3357 - accuracy: 0.8619\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3352 - accuracy: 0.8626\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.3348 - accuracy: 0.8633\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - 0s 704us/step - loss: 0.3349 - accuracy: 0.8622\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.3349 - accuracy: 0.8634\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - 0s 715us/step - loss: 0.3347 - accuracy: 0.8629\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3347 - accuracy: 0.8630\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3343 - accuracy: 0.8631\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 696us/step - loss: 0.3342 - accuracy: 0.8633\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - 0s 683us/step - loss: 0.3340 - accuracy: 0.8630\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3341 - accuracy: 0.8621\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - 0s 727us/step - loss: 0.3340 - accuracy: 0.8625\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - 0s 713us/step - loss: 0.3338 - accuracy: 0.8621\n",
      "Epoch 160/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3341 - accuracy: 0.8624\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3336 - accuracy: 0.8619\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.3334 - accuracy: 0.8635\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.3336 - accuracy: 0.8620\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3338 - accuracy: 0.8635\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3330 - accuracy: 0.8618\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3335 - accuracy: 0.8626\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - 0s 743us/step - loss: 0.3332 - accuracy: 0.8624\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3335 - accuracy: 0.8626\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3329 - accuracy: 0.8627\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3329 - accuracy: 0.8635\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3330 - accuracy: 0.8633\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3327 - accuracy: 0.8630\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3328 - accuracy: 0.8625\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3328 - accuracy: 0.8634\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.3324 - accuracy: 0.8631\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3324 - accuracy: 0.8618\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3326 - accuracy: 0.8631\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3325 - accuracy: 0.8644\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.3321 - accuracy: 0.8636\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.3322 - accuracy: 0.8627\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3321 - accuracy: 0.8624\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3321 - accuracy: 0.8612\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3317 - accuracy: 0.8630\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - 0s 724us/step - loss: 0.3320 - accuracy: 0.8627\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3318 - accuracy: 0.8625\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3318 - accuracy: 0.8621\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3322 - accuracy: 0.8616\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - 0s 745us/step - loss: 0.3320 - accuracy: 0.8621\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3319 - accuracy: 0.8637\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.3314 - accuracy: 0.8626\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3317 - accuracy: 0.8627\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3314 - accuracy: 0.8624\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3316 - accuracy: 0.8631\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3317 - accuracy: 0.8626\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - 0s 743us/step - loss: 0.3311 - accuracy: 0.8627\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - 0s 751us/step - loss: 0.3316 - accuracy: 0.8630\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.3314 - accuracy: 0.8630\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3314 - accuracy: 0.8620\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3314 - accuracy: 0.8620\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - 0s 754us/step - loss: 0.3312 - accuracy: 0.8621\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - 0s 725us/step - loss: 0.3312 - accuracy: 0.8643\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3311 - accuracy: 0.8633\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3312 - accuracy: 0.8620\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3309 - accuracy: 0.8633\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - 0s 743us/step - loss: 0.3308 - accuracy: 0.8614\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3308 - accuracy: 0.8631\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - 0s 729us/step - loss: 0.3308 - accuracy: 0.8616\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3308 - accuracy: 0.8614\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - 0s 730us/step - loss: 0.3307 - accuracy: 0.8624\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - 0s 733us/step - loss: 0.3304 - accuracy: 0.8637\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3305 - accuracy: 0.8622\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3308 - accuracy: 0.8618\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3309 - accuracy: 0.8629\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3306 - accuracy: 0.8615\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - 0s 817us/step - loss: 0.3305 - accuracy: 0.8630\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3305 - accuracy: 0.8619\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.3305 - accuracy: 0.8620\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3304 - accuracy: 0.8634\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - 0s 766us/step - loss: 0.3308 - accuracy: 0.8616\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3308 - accuracy: 0.8618\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3302 - accuracy: 0.8620\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3303 - accuracy: 0.8621\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.3304 - accuracy: 0.8622\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - 0s 717us/step - loss: 0.3301 - accuracy: 0.8612\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.3304 - accuracy: 0.8619\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - 0s 837us/step - loss: 0.3307 - accuracy: 0.8619\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3300 - accuracy: 0.8633\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3304 - accuracy: 0.8614\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - 0s 739us/step - loss: 0.3304 - accuracy: 0.8611\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3300 - accuracy: 0.8608\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3301 - accuracy: 0.8608\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3301 - accuracy: 0.8619\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3304 - accuracy: 0.8629\n",
      "Epoch 234/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 724us/step - loss: 0.3306 - accuracy: 0.8625\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3304 - accuracy: 0.8634\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3297 - accuracy: 0.8621\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3303 - accuracy: 0.8630\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3300 - accuracy: 0.8641\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3300 - accuracy: 0.8624\n",
      "Epoch 240/500\n",
      "80/80 [==============================] - 0s 727us/step - loss: 0.3300 - accuracy: 0.8629\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - 0s 715us/step - loss: 0.3298 - accuracy: 0.8620\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3299 - accuracy: 0.8616\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3300 - accuracy: 0.8626\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.3298 - accuracy: 0.8625\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3298 - accuracy: 0.8626\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - 0s 727us/step - loss: 0.3300 - accuracy: 0.8626\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3301 - accuracy: 0.8625\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3298 - accuracy: 0.8614\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3299 - accuracy: 0.8619\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - 0s 667us/step - loss: 0.3298 - accuracy: 0.8629\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3300 - accuracy: 0.8606\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3296 - accuracy: 0.8616\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - 0s 675us/step - loss: 0.3297 - accuracy: 0.8622\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - 0s 729us/step - loss: 0.3294 - accuracy: 0.8636\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3296 - accuracy: 0.8627\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - 0s 679us/step - loss: 0.3296 - accuracy: 0.8626\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3294 - accuracy: 0.8625\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3296 - accuracy: 0.8625\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - 0s 683us/step - loss: 0.3293 - accuracy: 0.8635\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - 0s 665us/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3296 - accuracy: 0.8630\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3295 - accuracy: 0.8640\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3298 - accuracy: 0.8626\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3294 - accuracy: 0.8618\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - 0s 678us/step - loss: 0.3293 - accuracy: 0.8631\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3294 - accuracy: 0.8622\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - 0s 717us/step - loss: 0.3293 - accuracy: 0.8622\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3294 - accuracy: 0.8624\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3294 - accuracy: 0.8626\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3292 - accuracy: 0.8622\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - 0s 659us/step - loss: 0.3298 - accuracy: 0.8630\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.3293 - accuracy: 0.8629\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.3292 - accuracy: 0.8640\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - 0s 674us/step - loss: 0.3294 - accuracy: 0.8614\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - 0s 745us/step - loss: 0.3291 - accuracy: 0.8637\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3297 - accuracy: 0.8626\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - 0s 695us/step - loss: 0.3291 - accuracy: 0.8631\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - 0s 665us/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - 0s 662us/step - loss: 0.3296 - accuracy: 0.8624\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3293 - accuracy: 0.8631\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3289 - accuracy: 0.8633\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3296 - accuracy: 0.8625\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3291 - accuracy: 0.8624\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3292 - accuracy: 0.8627\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - 0s 740us/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3290 - accuracy: 0.8629\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3289 - accuracy: 0.8633\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.3292 - accuracy: 0.8626\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - 0s 665us/step - loss: 0.3290 - accuracy: 0.8624\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - 0s 640us/step - loss: 0.3290 - accuracy: 0.8630\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - 0s 679us/step - loss: 0.3290 - accuracy: 0.8629\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - 0s 652us/step - loss: 0.3291 - accuracy: 0.8627\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3291 - accuracy: 0.8640\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3292 - accuracy: 0.8631\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - 0s 702us/step - loss: 0.3293 - accuracy: 0.8641\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3290 - accuracy: 0.8635\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.3288 - accuracy: 0.8626\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3292 - accuracy: 0.8626\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3289 - accuracy: 0.8641\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3293 - accuracy: 0.8625\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3289 - accuracy: 0.8622\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - 0s 675us/step - loss: 0.3289 - accuracy: 0.8630\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.3290 - accuracy: 0.8627\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3286 - accuracy: 0.8635\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.3288 - accuracy: 0.8637\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.3289 - accuracy: 0.8625\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - 0s 717us/step - loss: 0.3289 - accuracy: 0.8635\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3286 - accuracy: 0.8626\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.3289 - accuracy: 0.8626\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3289 - accuracy: 0.8619\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 699us/step - loss: 0.3286 - accuracy: 0.8635\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3288 - accuracy: 0.8633\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - 0s 647us/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - 0s 660us/step - loss: 0.3291 - accuracy: 0.8631\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 318/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3287 - accuracy: 0.8648\n",
      "Epoch 319/500\n",
      "80/80 [==============================] - 0s 852us/step - loss: 0.3287 - accuracy: 0.8637\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - 0s 678us/step - loss: 0.3287 - accuracy: 0.8639\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3286 - accuracy: 0.8629\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3284 - accuracy: 0.8643\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - 0s 678us/step - loss: 0.3286 - accuracy: 0.8644\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - 0s 683us/step - loss: 0.3287 - accuracy: 0.8631\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3286 - accuracy: 0.8633\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - 0s 708us/step - loss: 0.3286 - accuracy: 0.8630\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - 0s 677us/step - loss: 0.3285 - accuracy: 0.8627\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - 0s 683us/step - loss: 0.3287 - accuracy: 0.8658\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3285 - accuracy: 0.8634\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3286 - accuracy: 0.8631\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - 0s 674us/step - loss: 0.3285 - accuracy: 0.8634\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - 0s 702us/step - loss: 0.3284 - accuracy: 0.8634\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3285 - accuracy: 0.8636\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - 0s 788us/step - loss: 0.3283 - accuracy: 0.8634\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3285 - accuracy: 0.8654\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3287 - accuracy: 0.8636\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - 0s 754us/step - loss: 0.3292 - accuracy: 0.8649\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3285 - accuracy: 0.8640\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - 0s 670us/step - loss: 0.3288 - accuracy: 0.8636\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - 0s 667us/step - loss: 0.3284 - accuracy: 0.8627\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3287 - accuracy: 0.8622\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3287 - accuracy: 0.8622\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - 0s 676us/step - loss: 0.3283 - accuracy: 0.8639\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3282 - accuracy: 0.8635\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.3282 - accuracy: 0.8639\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3283 - accuracy: 0.8649\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - 0s 740us/step - loss: 0.3282 - accuracy: 0.8637\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3282 - accuracy: 0.8645\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3283 - accuracy: 0.8650\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3284 - accuracy: 0.8637\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - 0s 708us/step - loss: 0.3285 - accuracy: 0.8654\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - 0s 689us/step - loss: 0.3281 - accuracy: 0.8639\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3285 - accuracy: 0.8625\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - 0s 677us/step - loss: 0.3284 - accuracy: 0.8633\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - 0s 688us/step - loss: 0.3284 - accuracy: 0.8631\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - 0s 677us/step - loss: 0.3281 - accuracy: 0.8641\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3281 - accuracy: 0.8634\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - 0s 688us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3286 - accuracy: 0.8637\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3283 - accuracy: 0.8631\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3282 - accuracy: 0.8639\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3286 - accuracy: 0.8639\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - 0s 681us/step - loss: 0.3283 - accuracy: 0.8636\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - 0s 692us/step - loss: 0.3282 - accuracy: 0.8637\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - 0s 665us/step - loss: 0.3279 - accuracy: 0.8664\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3281 - accuracy: 0.8637\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - 0s 735us/step - loss: 0.3281 - accuracy: 0.8643\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3280 - accuracy: 0.8654\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3281 - accuracy: 0.8650\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - 0s 687us/step - loss: 0.3283 - accuracy: 0.8633\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3282 - accuracy: 0.8634\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3281 - accuracy: 0.8640\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3281 - accuracy: 0.8645\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - 0s 686us/step - loss: 0.3286 - accuracy: 0.8627\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3284 - accuracy: 0.8631\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3280 - accuracy: 0.8633\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.3285 - accuracy: 0.8648\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - 0s 665us/step - loss: 0.3278 - accuracy: 0.8640\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3282 - accuracy: 0.8639\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.3281 - accuracy: 0.8635\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.3283 - accuracy: 0.8627\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - 0s 682us/step - loss: 0.3279 - accuracy: 0.8620\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3278 - accuracy: 0.8636\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 699us/step - loss: 0.3281 - accuracy: 0.8637\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - 0s 708us/step - loss: 0.3287 - accuracy: 0.8633\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3276 - accuracy: 0.8646\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - 0s 697us/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3281 - accuracy: 0.8641\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - 0s 665us/step - loss: 0.3280 - accuracy: 0.8646\n",
      "Epoch 398/500\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.3279 - accuracy: 0.8641\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - 0s 707us/step - loss: 0.3285 - accuracy: 0.8631\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - 0s 735us/step - loss: 0.3278 - accuracy: 0.8634\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.3279 - accuracy: 0.8639\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - 0s 695us/step - loss: 0.3284 - accuracy: 0.8630\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3283 - accuracy: 0.8630\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3279 - accuracy: 0.8646\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - 0s 738us/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - 0s 714us/step - loss: 0.3278 - accuracy: 0.8643\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3283 - accuracy: 0.8631\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.3283 - accuracy: 0.8630\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3278 - accuracy: 0.8641\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - 0s 707us/step - loss: 0.3282 - accuracy: 0.8646\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3281 - accuracy: 0.8633\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - 0s 810us/step - loss: 0.3278 - accuracy: 0.8639\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.3281 - accuracy: 0.8640\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3281 - accuracy: 0.8631\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - 0s 726us/step - loss: 0.3276 - accuracy: 0.8636\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3278 - accuracy: 0.8624\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - 0s 687us/step - loss: 0.3277 - accuracy: 0.8648\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - 0s 720us/step - loss: 0.3279 - accuracy: 0.8637\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3277 - accuracy: 0.8636\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.3276 - accuracy: 0.8646\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3276 - accuracy: 0.8648\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - 0s 675us/step - loss: 0.3278 - accuracy: 0.8635\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3283 - accuracy: 0.8643\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3283 - accuracy: 0.8633\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3277 - accuracy: 0.8640\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.3274 - accuracy: 0.8631\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3275 - accuracy: 0.8644\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3276 - accuracy: 0.8630\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - 0s 718us/step - loss: 0.3280 - accuracy: 0.8633\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.3275 - accuracy: 0.8640\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3274 - accuracy: 0.8637\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.3279 - accuracy: 0.8630\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.3276 - accuracy: 0.8639\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3279 - accuracy: 0.8649\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - 0s 678us/step - loss: 0.3278 - accuracy: 0.8631\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3277 - accuracy: 0.8633\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3280 - accuracy: 0.8633\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - 0s 726us/step - loss: 0.3277 - accuracy: 0.8645\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3280 - accuracy: 0.8643\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - 0s 718us/step - loss: 0.3277 - accuracy: 0.8626\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3275 - accuracy: 0.8640\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - 0s 733us/step - loss: 0.3276 - accuracy: 0.8645\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3277 - accuracy: 0.8643\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - 0s 717us/step - loss: 0.3279 - accuracy: 0.8644\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - 0s 730us/step - loss: 0.3275 - accuracy: 0.8640\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - 0s 693us/step - loss: 0.3272 - accuracy: 0.8635\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3278 - accuracy: 0.8637\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3276 - accuracy: 0.8630\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3277 - accuracy: 0.8641\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.3275 - accuracy: 0.8634\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3277 - accuracy: 0.8629\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3280 - accuracy: 0.8637\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - 0s 720us/step - loss: 0.3276 - accuracy: 0.8637\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3277 - accuracy: 0.8631\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.3277 - accuracy: 0.8639\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - 0s 756us/step - loss: 0.3276 - accuracy: 0.8640\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - 0s 735us/step - loss: 0.3276 - accuracy: 0.8633\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.3275 - accuracy: 0.8635\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - 0s 730us/step - loss: 0.3277 - accuracy: 0.8651\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3276 - accuracy: 0.8630\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3274 - accuracy: 0.8633\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3276 - accuracy: 0.8634\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - 0s 766us/step - loss: 0.3277 - accuracy: 0.8649\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.3271 - accuracy: 0.8640\n",
      "Epoch 471/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 734us/step - loss: 0.3274 - accuracy: 0.8631\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - 0s 737us/step - loss: 0.3276 - accuracy: 0.8644\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3274 - accuracy: 0.8637\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3273 - accuracy: 0.8636\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3284 - accuracy: 0.8637\n",
      "Epoch 476/500\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.3274 - accuracy: 0.8634\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3277 - accuracy: 0.8629\n",
      "Epoch 478/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3272 - accuracy: 0.8645\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3273 - accuracy: 0.8627\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - 0s 732us/step - loss: 0.3273 - accuracy: 0.8640\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.3279 - accuracy: 0.8633\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - 0s 749us/step - loss: 0.3273 - accuracy: 0.8637\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3274 - accuracy: 0.8635\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - 0s 727us/step - loss: 0.3276 - accuracy: 0.8634\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - 0s 749us/step - loss: 0.3275 - accuracy: 0.8636\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - 0s 776us/step - loss: 0.3274 - accuracy: 0.8645\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3273 - accuracy: 0.8654\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - 0s 737us/step - loss: 0.3276 - accuracy: 0.8641\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - 0s 766us/step - loss: 0.3277 - accuracy: 0.8641\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3273 - accuracy: 0.8625\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.3278 - accuracy: 0.8634\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.3275 - accuracy: 0.8643\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - 0s 729us/step - loss: 0.3276 - accuracy: 0.8639\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.3275 - accuracy: 0.8655\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.3273 - accuracy: 0.8645\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.3276 - accuracy: 0.8631\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.3278 - accuracy: 0.8643\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.3276 - accuracy: 0.8635\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3274 - accuracy: 0.8641\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.3272 - accuracy: 0.8640\n"
     ]
    }
   ],
   "source": [
    "#### Last step in creation of NNmodel NNmodel is trained on the training set here with Tensor-Keras .fit based on Compiler\n",
    "#Fitting NNmodel\n",
    "history=NNmodel.fit(X_train,\n",
    "                    Y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs = 500)\n",
    "### Note that tf.keras.models.Sequential() by default uses glorot initializer -- drawing intial weights from a uniform \n",
    "### distribution -- see other possibilities in https://keras.io/api/layers/initializers/\n",
    "### Or you could try own customized wts inputs using\n",
    "### for layer in model.layers:\n",
    "###    init_layer_weight = [] # the weights yourself in this layer\n",
    "###    layer.set_weights(init_layer_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a74ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (100, 6)                  78        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (100, 6)                  42        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (100, 6)                  42        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (100, 1)                  7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169 (676.00 Byte)\n",
      "Trainable params: 169 (676.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fbefe2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.797249972820282, 0.7973750233650208, 0.8015000224113464, 0.8087499737739563, 0.8153749704360962, 0.8201249837875366, 0.8211249709129333, 0.8206250071525574, 0.8228750228881836, 0.8240000009536743, 0.8247500061988831, 0.8271250128746033, 0.8270000219345093, 0.8287500143051147, 0.828499972820282, 0.8295000195503235, 0.8286250233650208, 0.8291249871253967, 0.8307499885559082, 0.8299999833106995, 0.8323749899864197, 0.8335000276565552, 0.8345000147819519, 0.8368750214576721, 0.8371250033378601, 0.8395000100135803, 0.8412500023841858, 0.8414999842643738, 0.8423749804496765, 0.8443750143051147, 0.8452500104904175, 0.8460000157356262, 0.8475000262260437, 0.8476250171661377, 0.8483750224113464, 0.8501250147819519, 0.8498749732971191, 0.8511250019073486, 0.8500000238418579, 0.8528749942779541, 0.8525000214576721, 0.8532500267028809, 0.8531249761581421, 0.8541250228881836, 0.8542500138282776, 0.8560000061988831, 0.8558750152587891, 0.8567500114440918, 0.8578749895095825, 0.8575000166893005, 0.8582500219345093, 0.8586249947547913, 0.8573750257492065, 0.8571249842643738, 0.8585000038146973, 0.8598750233650208, 0.859624981880188, 0.859375, 0.8585000038146973, 0.8613749742507935, 0.8604999780654907, 0.8607500195503235, 0.8616250157356262, 0.8613749742507935, 0.8617500066757202, 0.8615000247955322, 0.8619999885559082, 0.8612499833106995, 0.8628749847412109, 0.8629999756813049, 0.8633750081062317, 0.8612499833106995, 0.8629999756813049, 0.8615000247955322, 0.862625002861023, 0.8629999756813049, 0.8627499938011169, 0.8617500066757202, 0.8622499704360962, 0.8607500195503235, 0.8615000247955322, 0.8615000247955322, 0.8616250157356262, 0.862375020980835, 0.8619999885559082, 0.8603749871253967, 0.8622499704360962, 0.8613749742507935, 0.8618749976158142, 0.8621249794960022, 0.8627499938011169, 0.8619999885559082, 0.8613749742507935, 0.8632500171661377, 0.8621249794960022, 0.8604999780654907, 0.8628749847412109, 0.8628749847412109, 0.8622499704360962, 0.8615000247955322, 0.8621249794960022, 0.8615000247955322, 0.8637499809265137, 0.8632500171661377, 0.8627499938011169, 0.8631250262260437, 0.8632500171661377, 0.862625002861023, 0.862500011920929, 0.862375020980835, 0.8627499938011169, 0.8631250262260437, 0.862625002861023, 0.862500011920929, 0.8631250262260437, 0.8629999756813049, 0.8627499938011169, 0.8627499938011169, 0.8632500171661377, 0.8629999756813049, 0.862500011920929, 0.8627499938011169, 0.8618749976158142, 0.862625002861023, 0.8632500171661377, 0.8622499704360962, 0.8633750081062317, 0.8628749847412109, 0.8629999756813049, 0.8631250262260437, 0.8632500171661377, 0.8629999756813049, 0.8621249794960022, 0.862500011920929, 0.8621249794960022, 0.862375020980835, 0.8618749976158142, 0.8634999990463257, 0.8619999885559082, 0.8634999990463257, 0.8617500066757202, 0.862625002861023, 0.862375020980835, 0.862625002861023, 0.8627499938011169, 0.8634999990463257, 0.8632500171661377, 0.8629999756813049, 0.862500011920929, 0.8633750081062317, 0.8631250262260437, 0.8617500066757202, 0.8631250262260437, 0.8643749952316284, 0.8636249899864197, 0.8627499938011169, 0.862375020980835, 0.8612499833106995, 0.8629999756813049, 0.8627499938011169, 0.862500011920929, 0.8621249794960022, 0.8616250157356262, 0.8621249794960022, 0.8637499809265137, 0.862625002861023, 0.8627499938011169, 0.862375020980835, 0.8631250262260437, 0.862625002861023, 0.8627499938011169, 0.8629999756813049, 0.8629999756813049, 0.8619999885559082, 0.8619999885559082, 0.8621249794960022, 0.8642500042915344, 0.8632500171661377, 0.8619999885559082, 0.8632500171661377, 0.8613749742507935, 0.8631250262260437, 0.8616250157356262, 0.8613749742507935, 0.862375020980835, 0.8637499809265137, 0.8622499704360962, 0.8617500066757202, 0.8628749847412109, 0.8615000247955322, 0.8629999756813049, 0.8618749976158142, 0.8619999885559082, 0.8633750081062317, 0.8616250157356262, 0.8617500066757202, 0.8619999885559082, 0.8621249794960022, 0.8622499704360962, 0.8612499833106995, 0.8618749976158142, 0.8618749976158142, 0.8632500171661377, 0.8613749742507935, 0.8611249923706055, 0.8607500195503235, 0.8607500195503235, 0.8618749976158142, 0.8628749847412109, 0.862500011920929, 0.8633750081062317, 0.8621249794960022, 0.8629999756813049, 0.8641250133514404, 0.862375020980835, 0.8628749847412109, 0.8619999885559082, 0.8616250157356262, 0.862625002861023, 0.862500011920929, 0.862625002861023, 0.862625002861023, 0.8627499938011169, 0.862500011920929, 0.8613749742507935, 0.8618749976158142, 0.8628749847412109, 0.8606250286102295, 0.8616250157356262, 0.8622499704360962, 0.862500011920929, 0.8636249899864197, 0.8627499938011169, 0.862625002861023, 0.862500011920929, 0.862500011920929, 0.8634999990463257, 0.862500011920929, 0.8629999756813049, 0.8640000224113464, 0.862625002861023, 0.8617500066757202, 0.8631250262260437, 0.8622499704360962, 0.8622499704360962, 0.862375020980835, 0.862625002861023, 0.8622499704360962, 0.8629999756813049, 0.8628749847412109, 0.8640000224113464, 0.8613749742507935, 0.8637499809265137, 0.862625002861023, 0.8631250262260437, 0.862500011920929, 0.862375020980835, 0.8631250262260437, 0.8632500171661377, 0.862500011920929, 0.862375020980835, 0.8627499938011169, 0.8636249899864197, 0.8628749847412109, 0.8632500171661377, 0.862625002861023, 0.862375020980835, 0.8629999756813049, 0.8628749847412109, 0.8627499938011169, 0.8640000224113464, 0.8631250262260437, 0.8641250133514404, 0.8634999990463257, 0.862625002861023, 0.862625002861023, 0.8641250133514404, 0.862500011920929, 0.8622499704360962, 0.8629999756813049, 0.8627499938011169, 0.8634999990463257, 0.8637499809265137, 0.862500011920929, 0.8634999990463257, 0.862625002861023, 0.862625002861023, 0.8618749976158142, 0.8634999990463257, 0.8632500171661377, 0.8632500171661377, 0.8631250262260437, 0.8632500171661377, 0.8647500276565552, 0.8637499809265137, 0.8638749718666077, 0.8632500171661377, 0.8636249899864197, 0.8638749718666077, 0.8638749718666077, 0.8628749847412109, 0.8642500042915344, 0.8643749952316284, 0.8631250262260437, 0.8638749718666077, 0.8632500171661377, 0.8629999756813049, 0.8627499938011169, 0.8657500147819519, 0.8633750081062317, 0.8631250262260437, 0.8633750081062317, 0.8633750081062317, 0.8636249899864197, 0.8633750081062317, 0.8653749823570251, 0.8636249899864197, 0.8648750185966492, 0.8640000224113464, 0.8636249899864197, 0.8627499938011169, 0.8622499704360962, 0.8622499704360962, 0.8638749718666077, 0.8634999990463257, 0.8636249899864197, 0.8638749718666077, 0.8648750185966492, 0.8637499809265137, 0.8644999861717224, 0.8650000095367432, 0.8637499809265137, 0.8653749823570251, 0.8638749718666077, 0.862500011920929, 0.8632500171661377, 0.8636249899864197, 0.8631250262260437, 0.8641250133514404, 0.8633750081062317, 0.8636249899864197, 0.8637499809265137, 0.8631250262260437, 0.8638749718666077, 0.8638749718666077, 0.8636249899864197, 0.8637499809265137, 0.8663750290870667, 0.8637499809265137, 0.8642500042915344, 0.8653749823570251, 0.8650000095367432, 0.8632500171661377, 0.8633750081062317, 0.8640000224113464, 0.8644999861717224, 0.8627499938011169, 0.8631250262260437, 0.8632500171661377, 0.8647500276565552, 0.8640000224113464, 0.8638749718666077, 0.8634999990463257, 0.8627499938011169, 0.8619999885559082, 0.8637499809265137, 0.8636249899864197, 0.8637499809265137, 0.8632500171661377, 0.8646249771118164, 0.8637499809265137, 0.8641250133514404, 0.8646249771118164, 0.8641250133514404, 0.8631250262260437, 0.8633750081062317, 0.8638749718666077, 0.8629999756813049, 0.8629999756813049, 0.8632500171661377, 0.8637499809265137, 0.8637499809265137, 0.8646249771118164, 0.8640000224113464, 0.8642500042915344, 0.8631250262260437, 0.8629999756813049, 0.8641250133514404, 0.8646249771118164, 0.8632500171661377, 0.8638749718666077, 0.8640000224113464, 0.8631250262260437, 0.8636249899864197, 0.862375020980835, 0.8647500276565552, 0.8637499809265137, 0.8636249899864197, 0.8646249771118164, 0.8640000224113464, 0.8647500276565552, 0.8634999990463257, 0.8642500042915344, 0.8632500171661377, 0.8640000224113464, 0.8631250262260437, 0.8643749952316284, 0.8629999756813049, 0.8632500171661377, 0.8640000224113464, 0.8637499809265137, 0.8629999756813049, 0.8638749718666077, 0.8648750185966492, 0.8648750185966492, 0.8631250262260437, 0.8632500171661377, 0.8632500171661377, 0.8644999861717224, 0.8642500042915344, 0.862625002861023, 0.8640000224113464, 0.8644999861717224, 0.8642500042915344, 0.8643749952316284, 0.8640000224113464, 0.8634999990463257, 0.8637499809265137, 0.8629999756813049, 0.8641250133514404, 0.8633750081062317, 0.8628749847412109, 0.8637499809265137, 0.8637499809265137, 0.8631250262260437, 0.8638749718666077, 0.8640000224113464, 0.8632500171661377, 0.8634999990463257, 0.8651250004768372, 0.8629999756813049, 0.8648750185966492, 0.8632500171661377, 0.8633750081062317, 0.8648750185966492, 0.8640000224113464, 0.8631250262260437, 0.8643749952316284, 0.8637499809265137, 0.8636249899864197, 0.8637499809265137, 0.8633750081062317, 0.8628749847412109, 0.8644999861717224, 0.8627499938011169, 0.8640000224113464, 0.8632500171661377, 0.8637499809265137, 0.8634999990463257, 0.8633750081062317, 0.8636249899864197, 0.8644999861717224, 0.8653749823570251, 0.8641250133514404, 0.8641250133514404, 0.862500011920929, 0.8633750081062317, 0.8642500042915344, 0.8638749718666077, 0.8654999732971191, 0.8644999861717224, 0.8631250262260437, 0.8642500042915344, 0.8634999990463257, 0.8641250133514404, 0.8640000224113464]\n",
      "[0.5893016457557678, 0.5326760411262512, 0.5103490352630615, 0.503338098526001, 0.5006687045097351, 0.4986584484577179, 0.49641796946525574, 0.4936690926551819, 0.4901726245880127, 0.48568347096443176, 0.48010972142219543, 0.4735147953033447, 0.4662652313709259, 0.45885518193244934, 0.4521009624004364, 0.4459957480430603, 0.4408891499042511, 0.43692195415496826, 0.43379640579223633, 0.43146440386772156, 0.4297277331352234, 0.428415447473526, 0.42740386724472046, 0.4265585243701935, 0.4256288707256317, 0.4247536063194275, 0.4238204061985016, 0.4228481650352478, 0.42195960879325867, 0.4207391142845154, 0.4195077121257782, 0.418244332075119, 0.41679707169532776, 0.4153968393802643, 0.4138506054878235, 0.4120216965675354, 0.4102395176887512, 0.4081364572048187, 0.40610170364379883, 0.4039492607116699, 0.40191763639450073, 0.39981895685195923, 0.39781054854393005, 0.3958269953727722, 0.39393889904022217, 0.3920172154903412, 0.3901054859161377, 0.38828614354133606, 0.38661280274391174, 0.3848974406719208, 0.3831782937049866, 0.38159456849098206, 0.3800228238105774, 0.37843379378318787, 0.3770548105239868, 0.3756897449493408, 0.37427306175231934, 0.37344470620155334, 0.3719380795955658, 0.37075889110565186, 0.3695344924926758, 0.3685770630836487, 0.36755648255348206, 0.36672958731651306, 0.3655540943145752, 0.3649452328681946, 0.3639896810054779, 0.3632229268550873, 0.3624051511287689, 0.36168432235717773, 0.3609168231487274, 0.3603725731372833, 0.35973215103149414, 0.3588700294494629, 0.35828647017478943, 0.3575718104839325, 0.3573157787322998, 0.35677292943000793, 0.3558677136898041, 0.3553571403026581, 0.35489416122436523, 0.35425469279289246, 0.35404521226882935, 0.353183776140213, 0.3527449071407318, 0.3522794544696808, 0.35148531198501587, 0.35104671120643616, 0.3507832884788513, 0.35046878457069397, 0.34970584511756897, 0.349311888217926, 0.34874680638313293, 0.3485272228717804, 0.34800252318382263, 0.3479425609111786, 0.34734800457954407, 0.3470546007156372, 0.3463990390300751, 0.3460821807384491, 0.34562116861343384, 0.34523260593414307, 0.3450617492198944, 0.3447292149066925, 0.34436339139938354, 0.3441086709499359, 0.34388455748558044, 0.3434159755706787, 0.34339988231658936, 0.34292861819267273, 0.3428129255771637, 0.34165331721305847, 0.3421768546104431, 0.3417413830757141, 0.34136125445365906, 0.3410200774669647, 0.3412899971008301, 0.3405963182449341, 0.34033042192459106, 0.3401812016963959, 0.3397934138774872, 0.33977821469306946, 0.3393953740596771, 0.339042067527771, 0.3389439880847931, 0.33881163597106934, 0.33869093656539917, 0.3385627269744873, 0.3378196656703949, 0.33842289447784424, 0.3378240466117859, 0.337865948677063, 0.3373604714870453, 0.3375045657157898, 0.3370955288410187, 0.33680734038352966, 0.33672046661376953, 0.3365615904331207, 0.3369047939777374, 0.33640074729919434, 0.3362410366535187, 0.3361591696739197, 0.33582597970962524, 0.3357301950454712, 0.3354988992214203, 0.3354959785938263, 0.3356790244579315, 0.33522796630859375, 0.33482107520103455, 0.33492615818977356, 0.3348923921585083, 0.3347310423851013, 0.33467018604278564, 0.33434292674064636, 0.3341611921787262, 0.33404502272605896, 0.3341163694858551, 0.3339810371398926, 0.3338017165660858, 0.3340528905391693, 0.33359989523887634, 0.333418071269989, 0.3336278200149536, 0.33378762006759644, 0.3329910337924957, 0.3335469365119934, 0.333177387714386, 0.33352819085121155, 0.33291345834732056, 0.33293092250823975, 0.3329809010028839, 0.33270564675331116, 0.3328336179256439, 0.33277031779289246, 0.3324439525604248, 0.33239513635635376, 0.3325516879558563, 0.33254897594451904, 0.33209267258644104, 0.33217230439186096, 0.3320610523223877, 0.3321189284324646, 0.33172523975372314, 0.3319867253303528, 0.3317538797855377, 0.33181676268577576, 0.3321667015552521, 0.331963449716568, 0.33188700675964355, 0.33142390847206116, 0.3316993713378906, 0.3313586115837097, 0.3316454589366913, 0.33168309926986694, 0.3310859799385071, 0.3315502405166626, 0.33140841126441956, 0.33144620060920715, 0.33143219351768494, 0.3311767876148224, 0.3311811089515686, 0.33109724521636963, 0.3312057852745056, 0.330911785364151, 0.33081114292144775, 0.3307912349700928, 0.33080145716667175, 0.33083653450012207, 0.3306802809238434, 0.3304101824760437, 0.3304634690284729, 0.33084794878959656, 0.3309191167354584, 0.33055758476257324, 0.3305272161960602, 0.3304949998855591, 0.3304835557937622, 0.33038219809532166, 0.3307885527610779, 0.3308214545249939, 0.3301893472671509, 0.33034390211105347, 0.3303581476211548, 0.33006787300109863, 0.33035364747047424, 0.3307022750377655, 0.3299866020679474, 0.3304193615913391, 0.33044907450675964, 0.33003801107406616, 0.3301408290863037, 0.3301490843296051, 0.33039236068725586, 0.33062270283699036, 0.3304263651371002, 0.32968297600746155, 0.33027365803718567, 0.3299613893032074, 0.3300054371356964, 0.32998374104499817, 0.3298417925834656, 0.32993030548095703, 0.3300469219684601, 0.3297904431819916, 0.3297695219516754, 0.330025851726532, 0.32960590720176697, 0.3300984799861908, 0.3297945261001587, 0.32985037565231323, 0.32977771759033203, 0.3299518823623657, 0.32956668734550476, 0.32972490787506104, 0.3292553126811981, 0.3293752074241638, 0.3295503556728363, 0.32957497239112854, 0.3294038474559784, 0.3296287953853607, 0.3293432593345642, 0.32929956912994385, 0.3295655846595764, 0.32952895760536194, 0.32979345321655273, 0.3294098377227783, 0.32926538586616516, 0.32943618297576904, 0.3292994201183319, 0.32944709062576294, 0.3293522298336029, 0.32917270064353943, 0.3297501504421234, 0.3293074369430542, 0.32923778891563416, 0.3293907344341278, 0.32909873127937317, 0.3296632766723633, 0.3290734589099884, 0.32932716608047485, 0.3295953869819641, 0.3293035924434662, 0.32889246940612793, 0.329559862613678, 0.32909703254699707, 0.32924407720565796, 0.3287114202976227, 0.32902610301971436, 0.32889389991760254, 0.32915908098220825, 0.32897138595581055, 0.328995019197464, 0.3290306031703949, 0.32913878560066223, 0.32906463742256165, 0.3291974663734436, 0.32930248975753784, 0.3290154039859772, 0.3288268446922302, 0.3292127549648285, 0.328923761844635, 0.32932740449905396, 0.3288581073284149, 0.3288768529891968, 0.32903873920440674, 0.32861223816871643, 0.3288057744503021, 0.3289482295513153, 0.3289487957954407, 0.3286445438861847, 0.3288867473602295, 0.3289080262184143, 0.32860538363456726, 0.32876962423324585, 0.32861432433128357, 0.3291146755218506, 0.3286789655685425, 0.32868388295173645, 0.3287123739719391, 0.328671932220459, 0.3286444842815399, 0.32868197560310364, 0.3285931944847107, 0.3286491334438324, 0.3286229372024536, 0.32838305830955505, 0.32859382033348083, 0.3287105858325958, 0.3285985291004181, 0.32860422134399414, 0.32856979966163635, 0.3284861147403717, 0.32865458726882935, 0.32849135994911194, 0.32864758372306824, 0.3285087049007416, 0.32835114002227783, 0.3285225033760071, 0.32825273275375366, 0.32850348949432373, 0.3286837637424469, 0.32924672961235046, 0.3284817636013031, 0.32880374789237976, 0.32839611172676086, 0.32870569825172424, 0.32873108983039856, 0.328326016664505, 0.328208863735199, 0.3283059298992157, 0.32824912667274475, 0.3283251225948334, 0.3282422423362732, 0.32820582389831543, 0.32827481627464294, 0.32839277386665344, 0.32851508259773254, 0.328063040971756, 0.32851293683052063, 0.32837381958961487, 0.3283122479915619, 0.32840242981910706, 0.32809513807296753, 0.3281013071537018, 0.3282659351825714, 0.3286396861076355, 0.3283092975616455, 0.3282173275947571, 0.3286442458629608, 0.3282546103000641, 0.32820093631744385, 0.3279319703578949, 0.3281398415565491, 0.32813170552253723, 0.32804128527641296, 0.3280680179595947, 0.32826322317123413, 0.3281598687171936, 0.3280605375766754, 0.3281145691871643, 0.3285534977912903, 0.3284275233745575, 0.32796597480773926, 0.32846805453300476, 0.3278278112411499, 0.32820332050323486, 0.32813504338264465, 0.3283429741859436, 0.32794293761253357, 0.3279390037059784, 0.32778942584991455, 0.3280739486217499, 0.32869207859039307, 0.32756802439689636, 0.32803109288215637, 0.3280968964099884, 0.32801029086112976, 0.32787758111953735, 0.3284718096256256, 0.32779747247695923, 0.3279268443584442, 0.3284376561641693, 0.328280508518219, 0.3278687298297882, 0.32799142599105835, 0.32793405652046204, 0.3279482424259186, 0.32774120569229126, 0.3277896046638489, 0.32825323939323425, 0.3283114433288574, 0.32782459259033203, 0.3281917870044708, 0.3280635178089142, 0.32777637243270874, 0.3280503451824188, 0.32806316018104553, 0.3276181221008301, 0.3278172016143799, 0.3276766538619995, 0.3279102146625519, 0.32765862345695496, 0.32756662368774414, 0.3277151584625244, 0.327619731426239, 0.32784879207611084, 0.32832565903663635, 0.3282890319824219, 0.32774922251701355, 0.3273782730102539, 0.3275235593318939, 0.3276342749595642, 0.32798221707344055, 0.3274752199649811, 0.3273589313030243, 0.32792285084724426, 0.3276039659976959, 0.32771843671798706, 0.3278649151325226, 0.3277891278266907, 0.3277023732662201, 0.32796403765678406, 0.3277052342891693, 0.32798606157302856, 0.3277134895324707, 0.3275153636932373, 0.3275858461856842, 0.3276689052581787, 0.3278818726539612, 0.32746970653533936, 0.3271884620189667, 0.32779017090797424, 0.32756689190864563, 0.3277299106121063, 0.3275435268878937, 0.32771191000938416, 0.32797420024871826, 0.3276424705982208, 0.3277255892753601, 0.32770836353302, 0.32760587334632874, 0.32757461071014404, 0.32751360535621643, 0.3276951014995575, 0.32758572697639465, 0.32771193981170654, 0.327419638633728, 0.3275621235370636, 0.32774385809898376, 0.32708320021629333, 0.32740575075149536, 0.32756567001342773, 0.32744452357292175, 0.3273259997367859, 0.3283930718898773, 0.32743027806282043, 0.32767513394355774, 0.3271813988685608, 0.3273002803325653, 0.32731714844703674, 0.32788699865341187, 0.3272692859172821, 0.32740235328674316, 0.3275510370731354, 0.3274751603603363, 0.3274230360984802, 0.3273482918739319, 0.3275696039199829, 0.32771649956703186, 0.32729434967041016, 0.3278375566005707, 0.3274725377559662, 0.3276413083076477, 0.3274914026260376, 0.32725775241851807, 0.3275904357433319, 0.3278224766254425, 0.3275875151157379, 0.32744699716567993, 0.3271850049495697]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history['accuracy'])\n",
    "print(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c57e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below are the weights in the final iteration\n",
    "first_layer_weights = NNmodel.layers[0].get_weights()[0]\n",
    "first_layer_biases  = NNmodel.layers[0].get_weights()[1]\n",
    "second_layer_weights = NNmodel.layers[1].get_weights()[0]\n",
    "second_layer_biases  = NNmodel.layers[1].get_weights()[1]\n",
    "third_layer_weights = NNmodel.layers[2].get_weights()[0]\n",
    "third_layer_biases  = NNmodel.layers[2].get_weights()[1]\n",
    "fourth_layer_weights = NNmodel.layers[3].get_weights()[0]\n",
    "fourth_layer_biases  = NNmodel.layers[3].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0f6f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.03574967e+00  1.67490557e-01  1.86995149e-01  3.36509019e-01\n",
      "   3.79849851e-01 -1.69410408e-01]\n",
      " [-1.04325938e+00  7.36042932e-02 -3.56880017e-02  5.67914069e-01\n",
      "   5.10322332e-01 -1.68643996e-01]\n",
      " [ 8.10043156e-01  3.93642128e-01 -7.13650286e-01  2.27512211e-01\n",
      "   5.12643039e-01  7.42869079e-02]\n",
      " [ 9.14491042e-02  1.44293644e-02 -9.75098014e-02 -3.92602906e-02\n",
      "  -5.40375113e-02 -6.20480850e-02]\n",
      " [ 1.19302310e-01  9.41861654e-04  3.41549605e-01  1.05293855e-01\n",
      "  -2.01200619e-02 -5.82128167e-02]\n",
      " [ 8.12340807e-03 -2.12230253e+00 -4.27198982e+00  3.30789447e-01\n",
      "   9.44071770e-01  1.05584037e+00]\n",
      " [ 2.10827515e-01  2.31868163e-01 -6.51495636e-01  7.86039233e-02\n",
      "   1.52221560e-01  3.33293617e-01]\n",
      " [-2.56467605e+00 -1.42363340e-01  1.34121850e-01  1.55861184e-01\n",
      "   1.98427126e-01 -2.92146623e-01]\n",
      " [-1.82030594e+00 -4.34886783e-01 -6.22622013e-01 -8.05621684e-01\n",
      "   3.78203821e+00  6.78933442e-01]\n",
      " [-1.36345699e-01  5.77987656e-02 -8.14865083e-02  3.79989333e-02\n",
      "   1.44718140e-01  9.96895358e-02]\n",
      " [ 1.26131162e-01 -4.54167306e-01 -9.53733385e-01  5.99399686e-01\n",
      "   5.76654553e-01  5.54939985e-01]\n",
      " [-2.59998202e-01 -1.29210517e-01 -4.02371585e-01  5.48575707e-02\n",
      "   6.99748248e-02  3.95225435e-02]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(first_layer_weights)\n",
    "first_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f38be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.164891    0.5052586  -0.9442029   0.12938133  1.1872071   0.6353613 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(first_layer_biases)\n",
    "first_layer_biases.shape  ### (2,) here basically means 2 elements in a 1-dim array. .T has no effect on 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "306e8073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13268632 -0.8023543   1.0004481   1.2128      1.0055463   1.1024324 ]\n",
      " [-2.0047786  -0.0628676  -0.12693255  0.9996308   1.5243332   0.17457227]\n",
      " [-0.6147839   0.7945753  -0.22524484 -0.7251248  -0.6961466  -0.21500689]\n",
      " [-1.4233338  -1.6550987   1.339142    2.1181324   1.9623063   1.822303  ]\n",
      " [-2.1103895  -1.4874996   2.1230435   1.1365488   1.4602978   1.4927187 ]\n",
      " [ 1.0954206   1.6544011  -2.0529993  -1.9984466  -1.7094976  -1.6852568 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(second_layer_weights)\n",
    "second_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5587208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1186947   0.79159796 -0.87981635 -0.7680316  -1.1356155  -1.1445026 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(second_layer_biases)\n",
    "second_layer_biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a4a041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.98501123e-01 -5.83123916e-01  1.74601919e+00 -3.24425289e-03\n",
      "   9.15090654e-01  2.01683866e+00 -1.04828130e+00  3.58411758e+00\n",
      "   8.02256955e-01  6.43760170e-01 -1.02327069e+00 -3.49886744e-01]]\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained weights and biases to try to predict based on a new case\n",
    "tr=sc.transform([[0, 0, 1, 650, 1, 60, 2, 300000, 2, 1, 0, 80000]])\n",
    "print(tr)  ### tr.shape is (1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8019c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e29f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78b364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cac86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a12d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ed4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461831bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86520ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "[[0.729483]]\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "### Predicting result for Single Observation\n",
    "print(NNmodel.predict(tr))\n",
    "### note in each recompute -- this no. will change slightly because of the random initiation of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a96e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045be36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9c8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9ae02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5835a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bcaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08bbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b2161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16fc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e89bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.88556794, -4.3574272 , -7.98658998, -0.28446817,  5.16841539,\n",
       "         1.10429385]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### now we compute the predicted prob of 1, manually\n",
    "tr.dot(first_layer_weights)  ### gives a 1 x 6 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "223820b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.72067694 -3.85216858 -8.93079288 -0.15508684  6.3556225   1.73965516]]\n"
     ]
    }
   ],
   "source": [
    "Flayerneurons_sum=tr.dot(first_layer_weights) + first_layer_biases\n",
    "print(Flayerneurons_sum)  ### 1 x 6 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7512768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.63150071e-04 2.07921466e-02 1.32235634e-04 4.61305815e-01\n",
      "  9.98266058e-01 8.50643260e-01]]\n"
     ]
    }
   ],
   "source": [
    "Flayerneurons_act=1/(1+np.exp(-Flayerneurons_sum))\n",
    "print(Flayerneurons_act)  ### 1 x 6 matrix -- output of neurons in hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f7f5c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75455881 -0.05085691  0.10842418 -0.33542544 -0.19503285 -0.24350444]]\n"
     ]
    }
   ],
   "source": [
    "Slayerneurons_sum=Flayerneurons_act.dot (second_layer_weights)+second_layer_biases\n",
    "print(Slayerneurons_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7960c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31982877 0.48728851 0.52707952 0.41692112 0.45139576 0.43942292]]\n"
     ]
    }
   ],
   "source": [
    "predprob=1/(1+np.exp(-Slayerneurons_sum))\n",
    "print(predprob) ### Note this is the same output as NNmodel.predict(tr)\n",
    "### This manual computation of the forward pass should have output same as in NNmodel.predict(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34a5ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 599us/step - loss: 0.3269 - accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32685568928718567, 0.8644999861717224]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample\n",
    "NNmodel.evaluate(X_train,Y_train)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ba646c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 552us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_train sample -- computing manually via .predict\n",
    "TE=NNmodel.predict(X_train)  ### note X_train has 8000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82ce3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f49c856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=(TE > 0.5).astype(int) ### Convert TE>0.5 == true ==> 1, False to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb651f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a697768",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "h[h==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9d41502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " ...\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8b950e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace all elements in numpy array of value 0 with value -1\n",
    "Y_train1=Y_train\n",
    "Y_train1[Y_train1==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53b50eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec6af287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe3de8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6916 0.8645\n"
     ]
    }
   ],
   "source": [
    "J=np.multiply(Y_train1.T,h.T)  ### element by element multiplication\n",
    "c=np.count_nonzero(J > 0) \n",
    "print(c,c/8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bfab32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 713us/step - loss: 0.3279 - accuracy: 0.8675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32790279388427734, 0.8675000071525574]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample\n",
    "NNmodel.evaluate(X_test,Y_test)  ### evaluates the loss and accuracy as specified in the Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f34f4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 581us/step\n"
     ]
    }
   ],
   "source": [
    "### Now we use the trained NNmodel to predict output in X_test sample -- computing manually via .predict\n",
    "TE1=NNmodel.predict(X_test)  ### note X_test has 2000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d186532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735 0.8675\n"
     ]
    }
   ],
   "source": [
    "h1=(TE1 > 0.5).astype(int) ### Convert TE1>0.5 == true ==> 1, False to 0\n",
    "h1[h1==0]=-1\n",
    "Y_test1=Y_test\n",
    "Y_test1[Y_test1==0]=-1\n",
    "J1=np.multiply(Y_test1.T,h1.T)  ### element by element multiplication\n",
    "c1=np.count_nonzero(J1 > 0) \n",
    "print(c1,c1/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2345cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
